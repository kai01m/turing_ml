{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kai01m/turing_ml/blob/main/TMDL_2024_25_Assignment_4%2C_Qs_only.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43554d37",
      "metadata": {
        "id": "43554d37"
      },
      "source": [
        "# Turing Machine and Deep Learning\n",
        "\n",
        "## Assignment Neural Networks\n",
        "\n",
        "This notebook is meant for you to review and reflect on the content of Lecture 4, which was mainly about neural networks. In particular, we'll focus on aspects that were only briefly mentioned in class in order for you to get a better understanding of it.\n",
        "\n",
        "### Handing in your Assignment\n",
        "\n",
        "Git is an invaluable resource to researchers and developers, and thus for this course, all course material will be (additionally) shared on GitHub. Though there is a tiny bit of a learning curve, this is worth the effort. To hand in your assignment (applicable to all weeks):\n",
        "\n",
        "1. Create a folder called \"Week 4\" and copy this notebook and any other files or data that may be needed.\n",
        "2. Finish the notebook and commit and push regularly. Your final commit before the deadline will be graded.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "52fbab8b",
      "metadata": {
        "id": "52fbab8b"
      },
      "outputs": [],
      "source": [
        "# load common libraries\n",
        "import numpy as np                 # maths\n",
        "import matplotlib.pyplot as plt    # plotting\n",
        "import pandas as pd                # data manipulation\n",
        "from tqdm import tqdm              # loading bar\n",
        "from time import perf_counter      # timer\n",
        "import tensorflow as tf            # NNs and associated\n",
        "from tensorflow import keras       # NNs and associated"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf4c3a0a",
      "metadata": {
        "id": "bf4c3a0a"
      },
      "source": [
        "# CNNs\n",
        "\n",
        "In the lecture, we saw some code that implemented MLPs and CNNs for the task of image classification on CIFAR10. However, we skipped over some details of what CNNs actually do.\n",
        "\n",
        "A CNN works by automatically learning and extracting meaningful patterns, features, and hierarchies from the input data in the following way:\n",
        "\n",
        "1. **Convolution**: A CNN applies (multiple) small filters (kernels) to an input image, performing element-wise multiplications and sums to extract features like edges, corners, and textures. We also did this computation in person during class. These kernels may be a matrix (what we saw in the lecture), or even a stack of matrices (e.g. for RGB images). I'd recommend watching this [3Blue1Brown video](https://www.youtube.com/watch?v=KuXjwB4LzSA&pp=ygUTY29udm9sdXRpb25zIGluIGNubg%3D%3D) and this [DeepLearning.AI](https://www.youtube.com/watch?v=KTB_OFoAQcc) video for reference.\n",
        "2. **Non-linearity**: Non-linear activation functions (e.g., ReLU) are applied to introduce complexity and allows the capture of more abstract representations.\n",
        "3. **Pooling**: Pooling operations are used to downsample the spatial dimensions of feature maps while preserving the most important information. In both cases, a pooling window or filter moves across the input feature map, and a single value is selected or computed within the window based on the pooling operation. Pooling helps reduce the computational complexity of the network and introduces some translation invariance. There are a few types of pooling, but most commonly used are max-pooling and average-pooling.\n",
        "4. **Hierarchy**: Convolution and pooling operations are repeated to create a hierarchy of feature extraction layers, learning more complex and high-level features.\n",
        "5. **Fully Connected Layers**: Flattened features are passed through fully connected layers, similar to a traditional MLP, for classification or regression.\n",
        "6. **Training**: Parameters are adjusted using backpropagation and optimization algorithms to minimize the difference between predictions and true labels.\n",
        "\n",
        "By leveraging convolution, non-linearity, and hierarchical feature extraction, CNNs can automatically learn and recognize intricate patterns and structures in visual data, making them well-suited for tasks like image classification and object detection.\n",
        "\n",
        "You can think of the convolution operation with a kernel over an image as outputting a new modified image, called a feature map. When applying convolutions over feature maps, the size of the output feature map (in each dimension) can be computed by:\n",
        "\n",
        "$$o = \\frac{i - k + 2 p}{s + 1}$$\n",
        "\n",
        "where the hyperparameters of the convolution are:\n",
        "- $i$: input_size is the size (height or width) of the input feature map.\n",
        "- $k$: kernel_size is the size (height or width) of the convolutional kernel/filter.\n",
        "- $p$: padding is the number of pixels added to each side of the input feature map (if applicable) (default 0)\n",
        "- $s$: stride is the step size or the number of pixels the kernel moves at each step (default 1)\n",
        "\n",
        "The output size of a pooling operation can also be defined as:\n",
        "$$o = \\frac{i - p}{s+1 }$$\n",
        "\n",
        "**Q1.1** Thus, starting with a 28x28 pixel image (e.g. MNIST), write a series of kernel sizes to use in convolutions (optionally including a pooling operation if you want to try it out, and optionally padding and stride) in order to get it down to an image size of 6x6. There are several correct answers possible here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5c54049",
      "metadata": {
        "id": "c5c54049"
      },
      "source": [
        "A possible sequence of operations is we ca start with a convolutional layer using a 3×3 kernel, stride 1, and padding 1, which maintains the spatial dimensions at 28×28. Next, apply a max pooling layer with a 2×2 kernel and stride 2, which reduces the size to 14×14. Follow this with another 3×3 convolution (again with stride 1 and padding 1) to preserve the 14×14 size. Then, apply a second max pooling layer with the same 2×2 kernel and stride 2 to reduce the dimensions further to 7×7. Finally, use a convolutional layer with a 2×2 kernel, stride 1, and no padding to reduce the size to the desired 6×6 output. This combination of layers effectively reduces the input size while preserving important features through the hierarchical feature extraction process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e1d363d",
      "metadata": {
        "id": "9e1d363d"
      },
      "source": [
        "**Q1.2** Now, let's try out your strategy in code. Implement the convolutions you defined above to perform classification over MNIST digits. Note that most of the code is written for you, but be mindful of the steps that are written. Note that you need to choose the number of kernals as well as the size at each layer. The input shape of your next layer is the output shape you computed above and the number of kernels you chose for the previous layer ``input_shape=(w,h,k)``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "58292f5b",
      "metadata": {
        "id": "58292f5b"
      },
      "outputs": [],
      "source": [
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d1918097",
      "metadata": {
        "id": "d1918097",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39696c8e-9273-4b6b-e634-c8a5d5e14aae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 146ms/step - accuracy: 0.8676 - loss: 0.4628 - val_accuracy: 0.9783 - val_loss: 0.0752\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9833 - loss: 0.0611\n",
            "Test accuracy: 0.9822166562080383\n"
          ]
        }
      ],
      "source": [
        "# TODO: Complete this cell\n",
        "\n",
        "# A Conv2D layer performs 2D convolution -- the same as we saw in class.\n",
        "# The main inputs to this class are the number of kernels, the size of each kernel,\n",
        "# the non-linear activation function and the input shape. Optionally, you can define a padding\n",
        "# and/or a stride. Use it like this:\n",
        "# keras.layers.Conv2D(<n_kernels>,\n",
        "#                     kernel_size=<tuple>,\n",
        "#                     activation=<string>,\n",
        "#                     input_shape=<tuple>,\n",
        "#                     padding=<string>,\n",
        "#                     stride=<int/tuple>)\n",
        "# You can read the documentation here: https://keras.io/api/layers/convolution_layers/convolution2d/\n",
        "\n",
        "# Build the model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, kernel_size=(3,3), activation=\"relu\", input_shape=(28, 28, 1)), # i'll give you the first shape for free\n",
        "    keras.layers.Conv2D(16, kernel_size=(2,2), padding=\"same\", activation=\"relu\", input_shape=(12, 12, 32)),\n",
        "    keras.layers.Flatten(),                            # flatten to dense layer for classification\n",
        "    keras.layers.Dense(128, activation='relu'),           # add dense layer (add more if you want)\n",
        "    keras.layers.Dense(10, activation='softmax')      # final dense layer (how many categories are there?).\n",
        "                                                       # softmax for probability distribution output\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model: choose batch size and number of epochs\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=1, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_train, y_train)\n",
        "print('Test accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4538a4e7",
      "metadata": {
        "id": "4538a4e7"
      },
      "source": [
        "**Q1.3** Plot the train and validation losses and and accuracies. Do not forget to add labels What can you say about the model over/underfitting? Would you continue training it, or stop earlier? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74e1aa1e",
      "metadata": {
        "id": "74e1aa1e"
      },
      "source": [
        "**Your text answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a1bf28d",
      "metadata": {
        "id": "0a1bf28d"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"Train loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Validation loss\")\n",
        "plt.ylabel(\"Loss (CCE)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.title(\"Loss curves for CNN model\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"accuracy\"], label=\"Train accuray\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"Validation accuracy\")\n",
        "plt.ylabel(\"Accuracy %\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy curves for CNN model\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecd9beef",
      "metadata": {
        "id": "ecd9beef"
      },
      "source": [
        "# RNNs\n",
        "\n",
        "An RNN (Recurrent Neural Network) is a type of artificial neural network that is particularly effective in processing sequential data. Unlike feedforward neural networks, which process inputs independently, RNNs have connections that allow information to flow in cycles. This cyclic connectivity enables them to capture temporal dependencies and learn patterns over time. You can find more in these [neural network lecture notes (Jaeger 2023, chapter 4)](https://www.ai.rug.nl/minds/uploads/LN_NN_RUG.pdf). Here is an overview of how RNNs work:\n",
        "\n",
        "- **Recurrent Connections**: RNNs have recurrent connections that allow information to be passed from one step to the next in a sequence. At each time step, the RNN takes an input and combines it with the information from the previous step. This feedback loop enables the network to have memory and make predictions based on the context of past inputs.\n",
        "\n",
        "- **Hidden State**: RNNs maintain a hidden state vector that serves as a memory of the network. The hidden state is updated at each time step and contains information about the previous inputs in the sequence. It captures the network's understanding of the sequence up to that point and is used to influence the processing of future inputs.\n",
        "\n",
        "- **Sequence Processing**: RNNs process sequences by iterating through each element one at a time. As the network receives an input at each time step, it updates its hidden state based on the input and the previous hidden state. The updated hidden state is then used to make predictions or generate outputs.\n",
        "\n",
        "- **Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU)**: LSTMs and GRUs are popular types of RNN architectures that address the vanishing gradient problem and capture long-term dependencies more effectively. These architectures introduce specialized memory cells and gating mechanisms that allow the network to selectively update and forget information.\n",
        "\n",
        "- **Training**: RNNs are typically trained using the backpropagation through time (BPTT) algorithm, which extends backpropagation to handle sequences. The goal is to minimize the difference between the predicted outputs and the true targets by adjusting the network's parameters through gradient descent optimization.\n",
        "\n",
        "- **Applications**: RNNs are well-suited for tasks involving sequential data, such as natural language processing, speech recognition, machine translation, time series analysis, and sentiment analysis. They can effectively model dependencies and capture context in these domains.\n",
        "\n",
        "By leveraging recurrent connections and hidden state information, RNNs excel at processing and understanding sequential data. They are powerful tools for tasks that require temporal modeling and have made significant contributions to the field of deep learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "da883fab",
      "metadata": {
        "id": "da883fab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "5aa42119-a5b0-40e9-9e00-fdfa7c655f5f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'AirPassengers.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-cbb0b47b9fa3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the Air Passengers dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AirPassengers.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtime_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'#Passengers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'AirPassengers.csv'"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Air Passengers dataset\n",
        "data = pd.read_csv('AirPassengers.csv')\n",
        "time_series = data['#Passengers'].values.astype(float)\n",
        "\n",
        "time_series = np.array(time_series).reshape(-1,1)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "time_series = scaler.fit_transform(time_series)\n",
        "\n",
        "# predict the (n+1)th point given n points\n",
        "sequence_length = 6\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data = time_series[:120]  # First 120 months for training\n",
        "test_data = time_series[120:]   # Last 24 months for testing\n",
        "\n",
        "# Function to create input sequences for the LSTM model\n",
        "def create_sequences(data, seq_length):\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Define sequence length and create input sequences\n",
        "X_train, y_train = create_sequences(train_data, sequence_length)\n",
        "X_test, y_test = create_sequences(test_data, sequence_length)\n",
        "\n",
        "\n",
        "# Reshape the input data to be 3D (batch_size, sequence_length, num_features)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c59cdc54",
      "metadata": {
        "scrolled": false,
        "id": "c59cdc54"
      },
      "outputs": [],
      "source": [
        "# Build the LSTM model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.LSTM(16, input_shape=(sequence_length, 1)),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b6f6172",
      "metadata": {
        "id": "1b6f6172"
      },
      "source": [
        "**Q1.3** Plot the train and validation losses and and accuracies. Do not forget to add labels What can you say about the model over/underfitting? Would you continue training it, or stop earlier? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "393976fa",
      "metadata": {
        "id": "393976fa"
      },
      "source": [
        "**Your text answer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7fd0655",
      "metadata": {
        "id": "d7fd0655",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "plt.plot(history.history[\"loss\"], label=\"Train loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"Validation loss\")\n",
        "plt.ylabel(\"Loss (MSE, normalized)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.title(\"Loss curves for LSTM model\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efec344c",
      "metadata": {
        "id": "efec344c"
      },
      "outputs": [],
      "source": [
        "# Predict on the test set\n",
        "# make predictions\n",
        "train_preds = model.predict(X_train)\n",
        "test_preds = model.predict(X_test)\n",
        "\n",
        "# invert predictions\n",
        "train_preds = scaler.inverse_transform(train_preds)\n",
        "y_train = scaler.inverse_transform(y_train)\n",
        "test_preds = scaler.inverse_transform(test_preds)\n",
        "y_test = scaler.inverse_transform(y_test)\n",
        "\n",
        "# plotting code adapted from https://www.kaggle.com/code/singhalamogh/lstm-regression-on-time-series-data\n",
        "# shift train predictions for plotting\n",
        "trainPredictPlot = np.empty_like(data)\n",
        "trainPredictPlot[:, :] = np.nan\n",
        "trainPredictPlot[sequence_length:len(train_preds) + sequence_length, :] = train_preds\n",
        "\n",
        "# shift test true data for plotting\n",
        "testTruePlot = np.empty_like(data)\n",
        "testTruePlot[:, :] = np.nan\n",
        "testTruePlot[len(train_preds)+sequence_length:len(data), :] = scaler.inverse_transform(test_data)\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = np.empty_like(data)\n",
        "testPredictPlot[:, :] = np.nan\n",
        "testPredictPlot[len(train_preds)+2*sequence_length:len(data), :] = test_preds\n",
        "\n",
        "# plot baseline and predictions\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "plt.plot(scaler.inverse_transform(train_data), color=colors[0], label=\"Train data\")\n",
        "plt.plot(testTruePlot[:,0], color=colors[1], label=\"Test data\")\n",
        "plt.plot(trainPredictPlot[:,0], color=colors[0], linestyle=\"--\", label=\"Predictions based on train data\")\n",
        "plt.plot(testPredictPlot[:,0], color=colors[1], linestyle=\"--\", label=\"Predictions based on test data\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# calculate root mean squared error\n",
        "train_score = mean_squared_error(y_train, train_preds)\n",
        "print('Train Score: %.2f (MSE)' % (train_score))\n",
        "test_score = mean_squared_error(y_test, test_preds)\n",
        "print('Test Score: %.2f (MSE)' % (test_score))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}